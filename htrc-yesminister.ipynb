{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "htrc-ef-groningen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrie-z/Collecting-data/blob/main/htrc-yesminister.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-AnFtjt4f_E"
      },
      "source": [
        "# Using Extracted Features to topic model with HTRC FeatureReader and Gensim\n",
        "This notebook will introduce [HTRC Extracted Features](https://wiki.htrc.illinois.edu/x/kYC2B) files, the [HTRC FeatureReader](https://github.com/htrc/htrc-feature-reader) Python library, and demo how EF files can be used to topic model a workset (collection) of volumes using [Gensim](https://radimrehurek.com/gensim/). This can be helpful both as exploratory data analysis, since anyone can download EF files whenever they'd like, as well as more robust analysis, as Gensim is a powerful topic modeling/machine learning library. This notebook will introduce a few new functions to reformat EF files to a format that can be fed directly into a topic modeling library, such as Gensim or MALLET, as well as some very entry-level visualization of our generated topics using [pyLDAvis](https://github.com/bmabey/pyLDAvis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUSKwBoT4f_H"
      },
      "source": [
        "Google Colaboratory (Colab) comes with a number of Python libraries built-in, but there are a few it doesn't have. These we will install directly on our Google server that runs this notebook, using `pip` a command line Python package manager. \n",
        "\n",
        "**NOTE: There will be a warning after running this cell that produces a \"restart runtime\" button. Do not worry about the warning, however, you will need to click on the button for the rest of the code in the notebook to work.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7rpivIETZMX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxi2ss-k7ZW8",
        "outputId": "6e0e95e6-78cd-4d30-b6b2-fec8e1cfee9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install htrc-feature-reader\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting htrc-feature-reader\n",
            "  Downloading htrc-feature-reader-2.0.7.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 30 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 40 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from htrc-feature-reader) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from htrc-feature-reader) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from htrc-feature-reader) (2.23.0)\n",
            "Collecting python-rapidjson\n",
            "  Downloading python_rapidjson-1.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 18.7 MB/s \n",
            "\u001b[?25hCollecting pymarc\n",
            "  Downloading pymarc-4.1.2-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 19.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->htrc-feature-reader) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->htrc-feature-reader) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->htrc-feature-reader) (2.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->htrc-feature-reader) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->htrc-feature-reader) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->htrc-feature-reader) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->htrc-feature-reader) (2.10)\n",
            "Building wheels for collected packages: htrc-feature-reader\n",
            "  Building wheel for htrc-feature-reader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htrc-feature-reader: filename=htrc_feature_reader-2.0.7-py3-none-any.whl size=40428 sha256=a142015e6337aae116c3462db01b6aee227c04b39382d643cb8bee740d53cfb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/18/9b/a096fa215c4f3703ed92bc7e0d50774c744685295415bc0ac9\n",
            "Successfully built htrc-feature-reader\n",
            "Installing collected packages: python-rapidjson, pymarc, htrc-feature-reader\n",
            "Successfully installed htrc-feature-reader-2.0.7 pymarc-4.1.2 python-rapidjson-1.5\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=f9b85386924c363bb11c4fd1524157371e9842404b9e33dbe454117cce10ee44\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.17 numpy-1.21.5 pandas-1.3.5 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N5R7W30FJkY"
      },
      "source": [
        "First, as almost always with a Jupyter notebook, we'll import the libraries that we'll be using. We need to do this in order to tell Python which libraries have the commands/methods we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTsqyQ0Z4f_K"
      },
      "source": [
        "import pandas as pd\n",
        "from htrc_features import FeatureReader, Volume\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import os\n",
        "import shutil\n",
        "import nltk\n",
        "import gensim\n",
        "\n",
        "# Because some of our libraries will throw many, many warnings for future changes to their code:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRMIKz4e4f_N"
      },
      "source": [
        "We've imported:\n",
        "- [Pandas](https://pandas.pydata.org/) a common library for working with large data, especially text data, while doing data science-y things.\n",
        "- the HTRC FeatureReader, a specialty Python library for working with EF files.\n",
        "- [TQDM](https://pypi.org/project/tqdm/) a library that allows for quick and easy progress bars for longer computational tasks.\n",
        "- `NLTK`, the [Natural Language Toolkit](https://www.nltk.org/)\n",
        "- Two built-in Python libraries, `shutil` and `os` which help us deal with files and file systems\n",
        "\n",
        "You can think of importing libraries as basically getting our tools into our workspace, because we know that for some tasks we'll need a screwdriver and for others we may need some duct tape or a drill.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un4Neu5w4f_O"
      },
      "source": [
        "## Working with Volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P5mo9yY4f_P"
      },
      "source": [
        "We'll first use the FeatureReader to quickly take a look at EF data. The HTRC FeatureReader is a Python library that was written to make working with the EF files, especially for exploratory analysis, easier. The FeatureReader library aggregates information at a collection and volume level, and also has functions/methods at both levels. For this notebook, we'll be using the EF data at the volume level, though iterating through multiple volumes.\n",
        "\n",
        "When using the FeatureReader, the first step is usually to create a `Volume` object out of one EF file. We're going to do this by giving the method `Volume()` a HathiTrust ID (HTID), but, if we had a lot of volumes we wanted to look at EF data for, we could also give it a file path to an EF file we already downloaded to our own computer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPoouYJB4f_Q"
      },
      "source": [
        "test_vol = Volume('mdp.39015018983885')\n",
        "print(f\"{test_vol.title}, by {test_vol.author}, {test_vol.pub_date}, {test_vol.id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ugAC5Zq4f_S"
      },
      "source": [
        "Above we see that we created a FeatureReader `Volume` and we use a few built-in methods for a `Volume` to return metadata about the book--author, title, pub date, and HTID. This metadata is bibliographic, and was assigned by a librarian or cataloger at the institution that submitted the item to HathiTrust. This is handy because it can be used as ground truth for machine learning tasks as well as used during the text analysis pipe line, such as for de-duplicating volumes. There are more metadata fields available than above, and you can see all the options by typing `test_vol.` below then hitting `Tab`, which will trigger a pop-up menu listing the possible methods available. See some samples and explore other fields below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndCu_FVY4f_T"
      },
      "source": [
        "# returns HT record number (which can help with de-duplicating)\n",
        "print(test_vol.oclc)\n",
        "\n",
        "# returns publisher information for the volume\n",
        "print(test_vol.publisher)\n",
        " \n",
        "# returns a code for the institution that submitted the item to HathiTrust\n",
        "print(test_vol.source_institution)\n",
        "\n",
        "# EXPLORE OTHER AVAILABLE METHODS/FIELDS\n",
        "print(test_vol.date_created)\n",
        "print(test_vol.genre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0_OgqPl4f_V"
      },
      "source": [
        "Some of the methods that pop-up in the above cell are more complex than just returning metadata fields in the file. Some of the methods will instead return new Pandas DataFrames (basically big, interactive tables of data that we'll be using a lot today) with information. For example, we can use `test_vol.begin_line_chars()` to return a DataFrame of the characters that start each line on a given page. Explore some of the other options in the cell below by commenting or un-commenting the code lines (by adding/removing a `#` in front of the line):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyVNSP-Y4f_V"
      },
      "source": [
        "# returns a DataFrame (DF) with begin-line characters for each page\n",
        "test_vol.begin_line_chars()\n",
        "\n",
        "# returns a DF with line counts for each page\n",
        "# test_vol.line_counts()\n",
        "\n",
        "# returns a DF with empty line counts for each page\n",
        "# test_vol.empty_line_counts()\n",
        "\n",
        "# EXPLORE OTHER METHODS!\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2IbuF04f_X"
      },
      "source": [
        "Beyond just generating these DataFrames, we can also plot many of them very quickly, which helps us better visualize a large amount of data. For instance, if we wondered about the textual structure of our volume, we may use `.tokens_per_page()` to visualize how many words are on each page of our volume, and see if there is anything interesting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWAQMoUl4f_X"
      },
      "source": [
        "tkns_df = test_vol.tokens_per_page()\n",
        "tkns_df.plot(figsize=(22,10))\n",
        "\n",
        "# if you want to compare to another volume, here is code for looking at Mrs. Dalloway by Virginia Woolf:\n",
        "# md_vol = Volume('mdp.39015002299421')\n",
        "# md_tkns = md_vol.tokens_per_page()\n",
        "# md_tkns.plot(figsize=(22,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZfGD5di4f_Y"
      },
      "source": [
        "Anything interesting here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spg7eNXe4f_Z"
      },
      "source": [
        "Similarly, we can use the FeatureReader for more complex exploratory analysis. Maybe we suspect that parts of our volume contained poetry, we may wish to see if there are any perceivable increases or decrease of capitalized begin line characters in our volume. We could check this with the FeatureReader and a handful of lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJitsZ94f_Z"
      },
      "source": [
        "# Create a new Volume from one anthology of Harper's Weekly, a publication known to publish both prose and poetry:\n",
        "blc_test_vol = Volume('coo.31924054824473')\n",
        "\n",
        "# create a DF with begin line characters only:\n",
        "blc_df = blc_test_vol.begin_line_chars()\n",
        "# blc_df.head(10)\n",
        "\n",
        "# print the shape of our blc DF\n",
        "print(f\"blc_df has {blc_df.shape[0]} rows and {blc_df.shape[1]} columns\")\n",
        "\n",
        "# make a new DF with only capitalized alphabetic begin line characters, by page:\n",
        "caps_blc_df = blc_df.loc[(slice(None), slice(None), slice(None), ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'],)]\n",
        "\n",
        "# plot the above df!\n",
        "caps_blc_df.plot(figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ2iFqJ54f_a"
      },
      "source": [
        "This plot is showing us the counts of capitalized begin line characters per page, and shows spikes around page 240-300, and then later in the volume. This shows where we are likely to have poetry in this volume, and happens to line up with more rigorous poetry-identifying work we've done in [another EF use case](https://wiki.htrc.illinois.edu/x/IQKGAQ)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB43QDm74f_b"
      },
      "source": [
        "## Topic Modeling with Token Lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMFBUZGm4f_c"
      },
      "source": [
        "At this point, you're probably wondering when we'll get to topic modeling using the tokens (words) themselves. Let's not delay any longer and dive in. \n",
        "\n",
        "Before we topic model, let's look a bit at how we can see and interact with tokens. The most flexible way to do this is to create a token list DataFrame (DF) using the method `vol.token_list()`. This method can be given extra options to get different versions of the data back, and these are called \"parameters.\" Let's take a look at the possible parameters first (maybe learning a handy trick), and then explore a token list. To see the possible parameters for a given function or method, you can type its name as if you were going to run it, but with a `?` instead of parentheses `()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvvTzbpV4f_d"
      },
      "source": [
        "# test_vol.tokenlist?\n",
        "\n",
        "tv_pf = test_vol.page_features()\n",
        "tv_pf.head(25) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggs6km5x4f_d"
      },
      "source": [
        "The above code will not only show us the parameters, but will return the entire Docstring help text for the function, which should not only tell you possible parameters, but also their default values, the data types the function and each parameter takes, the output of the function and even examples of how to use it and what it will return. One thing to keep in mind, though, is that this info is submitted by the author of the library, so how thorough or helpful this help text is can vary wildly between libraries!\n",
        "\n",
        "Ok, with a new trick under our belts, let's create our first token list to see what info is available. While we do this, we'll also assign the output of `.tokenlist()` to a variable and use `.head()` to look at the first 5 rows of data, since we know the output of `.tokenlist()` is a DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI9DKF9k4f_e"
      },
      "source": [
        "test_df = test_vol.tokenlist()\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XWcJDgk4f_f"
      },
      "source": [
        "Since we looked at an EF file, this data should look familiar, but the structure has been flattened a bit to fit into a DataFrame, which can make it easier to read to our human (hopefully!) eyes. Another interesting thing to note is that the DataFrame returned by `.tokenlist()` has a multi-index. This means that more columns than just a hidden numerical index are treated like an index--that is, you can easily search those levels of data for certain values.This is why we could use `.loc` to quickly find certain letters in our capitalized begin line character example above. This is a powerful way to find certain tokens or parts-of-speech or to do so on specific pages or sections of a volume, though we won't demo that here.\n",
        "\n",
        "Instead, let's turn directly to using Extracted Features files to topic model our data. Right now, we can get the tokens for each page in a given HathiTrust volume in a highly structured format, a token list. Most topic modeling tools simply want the tokens for each volume/document/page in bag-of-words format. You can think of the bag-of-words format exactly as it sounds:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1LHus0vtaiKxKIBIyimtSJOjvqaH4O6Oi)\n",
        "\n",
        "We take each word from a passage of text, drop it in a \"bag\" where it loses its original order but still represents the explicit pieces of the original text. Topic modeling is based on co-occurrence between words and word occurrence at the document and topic level, which is why the original semantic structure of the text is irrelevant.\n",
        "\n",
        "In order to convert EF files to bag-of-words text, we need to write a bit of custom code. We'll define a new function that will parse EF files, grab the tokens for each page and volume and add them to a large DataFrame that will have bag-of-words tokens as one of its columns, along with HTID and page number and a few others. We call this function `ef_vol_to_bow_df` and it takes a FeatureReader `Volume` as an input as well as an optional parameter to save the DataFrame to a TSV. Since this function takes one volume as an input, we'll need to wrap it in some iteration code in order to reformat multiple volumes. Here is the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ogwkAe4f_g"
      },
      "source": [
        "def ef_vol_to_bow_df(volume, save_to_tsv=False):\n",
        "    from htrc_features import FeatureReader, Volume\n",
        "    import pandas as pd\n",
        "    from tqdm.notebook import trange, tqdm\n",
        "    import os\n",
        "    notebook_path = os.getcwd()\n",
        "    htid = volume.id\n",
        "    # creating an empty Dataframe to which we'll add clean tokens, instead of a text file\n",
        "    vol_df = pd.DataFrame(columns=['htid', 'page_number', 'page_tokens'])\n",
        "    outfile_name = htid+'.tsv' # saving our volume-level token DataFrame to a TSV\n",
        "    for page in tqdm(volume.pages(), total=volume.page_count):\n",
        "        page_num = str(page).split(' ')[1]\n",
        "        page_df = page.tokenlist(section='body', case=False, pos=False)\n",
        "        tkn_list = []\n",
        "        \n",
        "        for i, r in page_df.iterrows():\n",
        "            tkn = i[2]\n",
        "            clean_tkn = tkn.strip()\n",
        "            count = r[0]\n",
        "            tkns = ([f'{clean_tkn}'] * count)\n",
        "            clean_tkn_list = [word for word in tkns if word.isalpha()]\n",
        "            clean_tkn_list = [word for word in clean_tkn_list if word not in en_stop]\n",
        "            tkn_list.extend(clean_tkn_list)\n",
        "        '''\n",
        "        Instead of writing to text files, we are adding the page-level clean tokens to our\n",
        "        DataFrame, with one each page of tokens constituting one row in the DataFrame.\n",
        "        '''\n",
        "        vol_df = vol_df.append({'htid': htid, 'page_number':  page_num, 'page_tokens':tkn_list}, ignore_index=True)\n",
        "    '''\n",
        "    Lastly, we save our volume-level DataFrame as a tab-separated file, and return the volume \n",
        "    DataFrame so that we can better aggregate each volume's tokens into a single DataFrame (you'll \n",
        "    see this code in the wrapper we write to iterate through multiple volumes)\n",
        "    '''\n",
        "    if save_to_tsv==True:\n",
        "        vol_df.to_csv(outfile_name, sep='\\t', index=False)\n",
        "        print(f'Saved {volume.title} to TSV named {outfile_name}')\n",
        "    \n",
        "    print(f'Reformatted \"{volume.title}\" ({htid}) to bag-of-words')\n",
        "    return vol_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCjhUGar4f_h"
      },
      "source": [
        "Before we run this function to test it out, we will download a standard set of stop words--the words we want to remove from the text before analysis--and decide if we want to add any additional words to this list. For this workshop, we've built removing stop words into the function that readies EF files for topic modeling. We'll use the standard English stop words from the Natural Language Toolkit (`nltk`) to start with. We have cheated (by running this already) to find that we should make some additions, which we'll do using `.add()` the syntax for adding items to a `Set` (a `Set` is basically a list, but where every item in it is unique and with some extra properties):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEJhkYae4f_h"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "en_stop.add(\"'\")\n",
        "en_stop.add('\"')\n",
        "en_stop.add(' ')\n",
        "en_stop.add('would')\n",
        "en_stop.add('could')\n",
        "en_stop.add('should')\n",
        "en_stop.add('said')\n",
        "en_stop.add('also')\n",
        "\n",
        "# If we wanted to be a bit more clever, we could use a simple loop to add these words to our stop list with less typing:\n",
        "\n",
        "# stop_words_to_add = [\"'\",'\"',' ','would','could','should','also', 'said']\n",
        "\n",
        "# for word in stop_words_to_add:\n",
        "#     en_stop.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57qtuoWi4f_h"
      },
      "source": [
        "With a list of stop words defined, we can write a few lines of code to iteratively deploy it and test the results. We'll first use three HTIDs for test volumes (the first three books of George RR Martin's *A Song of Ice and Fire* series) and then create a `for` loop to run our `ef_vol_to_bow_pages` function over all three volumes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5sgqxvK4f_i"
      },
      "source": [
        "# YOU COULD SUBSTITUTE ANY VOLUMES YOU'D LIKE HERE, JUST REPLACE THE BELOW HTIDs!\n",
        "volume_list = ['mdp.39015050507618','mdp.39015046463629','mdp.39015054095784']\n",
        "\n",
        "workset_page_df = pd.DataFrame(columns=['htid','page_number','page_tokens'])\n",
        "\n",
        "for book in tqdm(volume_list):\n",
        "    fr_vol = Volume(book)\n",
        "    book_df = ef_vol_to_bow_df(fr_vol, save_to_tsv=True)\n",
        "    workset_page_df = workset_page_df.append(book_df)\n",
        "    \n",
        "print(f\"Reformatted {len(volume_list)} volumes to bag-of-words pages.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdWYcyy4f_i"
      },
      "source": [
        "Things look to have run successfully (hopefully!). Let's check the dimensions (rows, columns) and look at the output DataFrame to verify:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw3Dai2d4f_i"
      },
      "source": [
        "print(workset_page_df.shape)\n",
        "workset_page_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CksqTLKx4f_j"
      },
      "source": [
        "The dimensions look right, as does the data in `workset_page_df`. \n",
        "\n",
        "We can now shift to grabbing the bag-of-words text in the `page_tokens` column and readying it for Gensim. We're using a straightforward, no frills version of topic modeling where we aren't tracking the documents in our collection, and are instead just topic modeling the words. For this, Gensim wants one big list of the tokens, and we'll generate this using another `for` loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWiAEJy64f_j"
      },
      "source": [
        "bow_text_lists = []\n",
        "bow_text = []\n",
        "\n",
        "for i,r in workset_page_df.iterrows():\n",
        "    words = r.page_tokens\n",
        "    bow_text_lists.append(words)\n",
        "    for word in words:\n",
        "        bow_text.append(word)\n",
        "\n",
        "print(f\"Added {len(bow_text)} words to list.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdP2pbYJ4f_k"
      },
      "source": [
        "Gensim takes \"dictionary\" and \"corpus\" objects to train an LDA model. For Gensim, a \"dictionary\" is a list of the unique tokens in your corpus, assigned to a numerical ID, and a \"corpus\" is your complete bag-of-words data--the list of words we just created, `bow_text`--but represented by the numerical ID from the \"dictionary\" and its count. This is a bit confusing, so let's generate both and take a look at them directly. Here is the code to create the dictionary and corpus, pulled from Gensim documentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgpVydM4f_k"
      },
      "source": [
        "# importing libraries from Gensim to create corpus and dictionary\n",
        "from gensim import corpora\n",
        "\n",
        "# creating our dictionary and corpus\n",
        "dictionary = corpora.Dictionary(bow_text_lists)\n",
        "corpus = [dictionary.doc2bow(text) for text in bow_text_lists]\n",
        "\n",
        "# importing pickle, a library which lets us save our corpus and dictionary for later use\n",
        "import pickle\n",
        "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "dictionary.save('dictionary.gensim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpNoKVRj4f_l"
      },
      "source": [
        "Here is a look at the first 15 items in our Gensim `dictionary`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-mArBkY4f_l"
      },
      "source": [
        "n = 0\n",
        "\n",
        "for k,v in dictionary.items():\n",
        "    if n <= 15:\n",
        "        print(f\"{k} : {v}\")\n",
        "        n += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5ZyYK4r4f_l"
      },
      "source": [
        "And here is one page of bag-of-words data (represented numerically as `(word ID, count)` pairs or tuples) in our Gensim `corpus`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8lEQFk34f_m",
        "scrolled": true
      },
      "source": [
        "# return item 5 in our corpus:\n",
        "corpus[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhXkHYds4f_m"
      },
      "source": [
        "With these two inputs created, we can now train and run our topic model, saving the model along the way (again, pulling code from Gensim documentation--you don't have to know how to do this off the top of your head!):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWWbJHDy4f_m"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# THIS CODE WILL TAKE A FEW MINUTES OR SO TO RUN!\n",
        "number_of_topics = 10\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = number_of_topics, id2word=dictionary, passes=50, random_state=42)\n",
        "ldamodel.save('model10.gensim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG_oI6zG4f_m"
      },
      "source": [
        "Three major inputs are adjustable: the number of topics we want to generate, the number of passes we want to do when training our model, and the random state or seed. The number of topics is custom for each set of volumes/documents and is often something that you should test out. Do your results make more or less sense with more or fewer topics? The number of passes will effect the \"accuracy\" of your topic models. The algorithm is training the topics as many times as you input passes, and then taking an aggregated average of the results. Since there is a random element to topic modeling, results are different for every pass. The more passes we do, the more we can reduce the randomness of the results. Here, we're doing 50 passes, since our data is not too big, but you may find more or less is suitable based on testing out results. Another way to minimize the randomness is to set the seed, or in Gensim the `random_state`. By setting the \"seed\" or the `random_state` we are saying to eliminate the randomness and always go with the number option we indicate. You can play with this number as well and see how the results change. We have started with 42 because it is the answer to eveything.\n",
        "\n",
        "Ok, back to results!\n",
        "\n",
        "Nothing is explicitly returned from the code above, as the model is created and saved, but we can see the topics that were generated using `show_topics()` which also lets us decide how many of the top words for each topic we want to see:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRICJeyF4f_n",
        "scrolled": true
      },
      "source": [
        "topics = ldamodel.show_topics(num_words=10, formatted=False)\n",
        "for topic in topics:\n",
        "    print(f\"{topic} + '\\n'\")\n",
        "\n",
        "# doc_topics = ldamodel.get_doc_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8cTOkII4VzB"
      },
      "source": [
        "\n",
        "We can also get all the documents for which a term has an association in our model using Gensim's `get_term_topics` method, which takes a term and a minimum probability and returns each topic for which that term meets the minimum probability of appearing in. We can see all topics for a given term simply by passing `0` to the parameter `minimum_probability`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dud9dUauIocf"
      },
      "source": [
        "ldamodel.get_term_topics('brave', minimum_probability=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxoEgYmF4xQp"
      },
      "source": [
        "While it's very useful to see our topics, the top N terms associated with them, and be able to check topic association for a given term, we may also want to see how each of our documents associates with our topics. We can use Gensim's `get_document_topics` method for this (and remember that a 'document' for us is a page of a HathiTrust volume):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f41fkJgIqmd"
      },
      "source": [
        "all_topics = ldamodel.get_document_topics(corpus, per_word_topics=True)\n",
        "n = 0\n",
        "\n",
        "for doc_topics, word_topics, phi_values in all_topics[20:25]:\n",
        "    print(f\"Document {n}\\n\")\n",
        "    print('Document topics (what topics this document is associated with):')\n",
        "    print(f\"{doc_topics}\\n\")\n",
        "    print('Word topics (what topics words in this document are associated with):')\n",
        "    print(f\"{word_topics}\\n\")\n",
        "    print('Phi values (probability of a word in this document being associated with a given topic):')\n",
        "    print(f\"{phi_values}\\n\")\n",
        "    print('-------------- \\n')\n",
        "    n += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MZ3yfFY4f_n"
      },
      "source": [
        "We can also Write out the topics and their top n words (here, n=25) to a file with the following code (**Note: this doesn't work in Google Colab**):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FjHRowD4f_n"
      },
      "source": [
        "# DOESN'T WORK FOR COLAB, SADLY!\n",
        "topics = ldamodel.show_topics(num_topics=5, num_words=25)\n",
        "\n",
        "with open('asoiaf_topics_5.tsv', 'a') as f:\n",
        "    for topic in topics:\n",
        "        topic_num = topic[0]\n",
        "        topic_words = topic[1]\n",
        "        f.write(f\"{topic_num} + '\\t' + {topic_words} + '\\n'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iir2OXJ4f_o"
      },
      "source": [
        "Now, with our topic modeling results, the fun begins--interpreting what we see. For this set of books, *A Game of Thrones*, *A Clash of Kings* and *A Storm of Swords*, the results *seem* to make sense, as we see the clustering of characters that interact with each other heavily, as well as thematic elements for those characters and their narrative arcs.\n",
        "\n",
        "We can use another library called `pyLDAvis` to better visualize our topics, and see if they still make sense:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpQQLnD4f_o"
      },
      "source": [
        "# load the dictionary, corpus, and LDA model we created earlier:\n",
        "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
        "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
        "\n",
        "# If you generate a new model and change the number of topics, you may need to change the file name for the model (here, model5.gensim)\n",
        "lda = gensim.models.ldamodel.LdaModel.load('model10.gensim')\n",
        "\n",
        "# import pyLDAvis and ready it for use in a notebook:\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# feed pyLDAvis the pieces generated from Gensim and create the visualization:\n",
        "lda_display = gensimvis.prepare(lda, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbSieLnJ4f_p"
      },
      "source": [
        "That's it! You've topic modeled 3 books, tens of thousands of words, using Gensim and HTRC Extracted Features. However, since topic modeling has a random seed, you should continue to explore by tweaking the inputs: stop words, number of topics, passes, and the random seed itself. We could also topic model while retaining the structure of the original pages/documents, or chunking our text into smaller pieces and reviewing how the results might change. Since topic modeling is really great for exploring data, we shouldn't be shy when it comes to experimenting with inputs and data. You'll never know how good results can be until you try different methods of generating them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqONZLwF4f_q"
      },
      "source": [
        "**Questions about this notebook?** Contact HTRC at htrc-help@hathitrust.org\n",
        "\n",
        "CC-BY-NC 4.0 (2021)"
      ]
    }
  ]
}